# -*- coding: utf-8 -*-
"""Kmeans_Teste+Erros.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ATLkMQgZHCB9-CwewQJORBUsa02VxzX_

# Importação e análise inicial dos dados
"""

# libraries
from sklearn import cluster
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os as os

# -------------------------------------------------------------------------------------------

# Em ordem, Definir:
# Porcentagem de dados utilizado para teste.
# Número de grupos.
# Legenda correta de agrupamento, caso exista (para cálculo de erro no final).
# Características utilizadas na classificação.
# Se os dados devem ser lidos ou importados do Seaborn
# Se os dados já estão agrupados ou não ( se não, não há como calcular erro)
# Nome do arquivo de entrada.

PorcentTeste = 0
numGrupos = 2
legenda_agrupamento =  'species' #'Heart Disease' 
selected_var =  ['sepal_width', 'petal_length', 'petal_width'] #['Max HR', 'Age', 'Chest pain type', 'Cholesterol',
                #'ST depression', 'Slope of ST']  #

#selected_var = ['BP', 'Cholesterol', 'Max HR', 'ST depression', 'Slope of ST']
#selected_var = ['Cholesterol']
dataRead = False
dataClustered = 1

fileName = 'dataSets/Heart_Disease_Prediction.txt'
# -------------------------------------------------------------------------------------------

# Leitura de dados
sns.set()
if (dataRead == False):
    data = sns.load_dataset("iris")
else:
    separator = ','
    data = pd.read_csv(fileName, sep=separator)

# Possibilidade de printar correlação
plotCorrelation = False
if (plotCorrelation):
    sns.pairplot(data, hue=legenda_agrupamento)
    plt.savefig('dataSets/Tabela_Correlação.png')

# Defino tamanho do teste
tamData = len(data)
numLinTeste = int(tamData * (PorcentTeste / 100))
print(" Número de objetos de treino: ", (tamData - numLinTeste))
print(" Número de objetos de teste: ", numLinTeste)
print("Número de variáveis / características:  ", len(selected_var))
# Crio função para obter lista de indexes pseudo-aleatórios para serem usados em teste, garantindo não repetição.


def getRandomIndexes(all_index, numLinTeste):
    random_Test_indexes = []
    for x in range(numLinTeste):
        tmp_int = np.random.choice(all_index, size=1)
        all_index.remove(tmp_int[0])
        random_Test_indexes.append(tmp_int[0])
    return random_Test_indexes


# Separo variáveis utilizadas para classificação, exceto 'species'
if (dataClustered == 1):
    data = data[selected_var + [legenda_agrupamento]]
else:
    data = data[selected_var]

# # Tratar nome de colunas ( retirando espaços )
columns_old = data.columns
columns_new = []
for column in columns_old:
    columns_new.append(column.replace(' ', '_'))
    if (column == legenda_agrupamento):
        legenda_agrupamento = column.replace(' ', '_')
data.columns = columns_new

# Obtenho Linha de indexes randômicos.
all_index = data.index.values.tolist()
random_Test_indexes = getRandomIndexes(all_index, numLinTeste)
# Separo DataFrames de Teste e Treino, além de armazenar agrupamento original do segmento de teste.
data_Teste = data.iloc[random_Test_indexes]
data_Treino = data.drop(random_Test_indexes, axis=0)

# Separo agrupamento original, se existente
if (dataClustered == 1):
    Agrupamento_original_treino = data_Treino[legenda_agrupamento]
    Agrupamento_original_teste = data_Teste[legenda_agrupamento]
    data_Treino.drop([legenda_agrupamento], axis=1, inplace=True)
    data_Teste.drop([legenda_agrupamento], axis=1, inplace=True)

""" Print de indexes de treino e teste

# Print indexes de Treino
print(" Indexes de Treino: ")
print(data_Treino.index.values)
# Print indexes de Teste
print(" Indexes de Teste: ")
print(data_Treino.index.values)

"""

# Cálculo de centroide original para cálculo de erro caso haja agrupamento original.
if (dataClustered == 1):
    num_grupos_dataset = numGrupos
    # Obtenho lista com nomes de cada tipo de objeto / label.
    group_labels = data[legenda_agrupamento].unique()

    # Crio dataset para centroide original
    Centroide_original = pd.DataFrame(
        columns=data_Teste.columns, index=group_labels)
    Centroide_original.fillna(0.0, inplace=True)

    # Calculando centroides do dataset original com classificação original (de especialista) ( Faz-se aqui apenas a soma dos valores encontrados, posteriormente a média)
    for item in data.index.values:  # Percorre index de todos os objetos.
        for group in group_labels:       # percorre nome dos grupos
            if (data[legenda_agrupamento][item] == group):
                # percorre nome das variáveis que dão dimensão à classificação.
                for dimension in data_Teste:
                    Centroide_original[dimension][group] += data[dimension][item]

    # determinar número de objetos em cada grupo no Dataset Original.
    agrup_original = data[legenda_agrupamento].to_numpy()
    num_objetos_original = pd.value_counts(agrup_original)
    Centroide_original = pd.concat([Centroide_original, pd.DataFrame(
        {'numObjetos': num_objetos_original})], axis=1)

    # Cálculo da média de cada variável, divindo pelo número de objetos classificados em cada grupo.
    for group in group_labels:
        for dimension in data_Teste:
            Centroide_original[dimension][group] = Centroide_original[dimension][group] / \
                Centroide_original['numObjetos'][group]

    # Substituir nomes de grupos por numeração do centroide original ( preparação para usar esses dados de agrupamento original no código de fortran).

    for numeracao in range(len(Centroide_original.index.values)):
        for nameGroup in Agrupamento_original_treino:
            Agrupamento_original_treino.replace(
                Centroide_original.index.values[numeracao], numeracao, inplace=True)
        for nameGroup in Agrupamento_original_teste:
            Agrupamento_original_teste.replace(
                Centroide_original.index.values[numeracao], numeracao, inplace=True)

    # Escrever em txt as informações do centroide e agrupamento original.
    Centroide_original.index.names = ['index']

    Agrupamento_original_treino.to_csv(
        'agrupamentoOriginal/agrup_original_treino.txt', sep='\t', index=False, header=True)

    Agrupamento_original_teste.to_csv(
        'agrupamentoOriginal/agrup_original_teste.txt', sep='\t', index=False, header=True)

    Centroide_original.drop(['numObjetos'], axis=1).to_csv(
        'Resultado_PlotKmeans/Centroide_Original.txt', sep='\t', index=True, header=False)


# Adicionar nome à coluna de indexes:
data_Treino.index.names = ['index']
data_Teste.index.names = ['index']

# Gerar tabela com argumentos
args = [numGrupos, len(selected_var), (tamData - numLinTeste), dataClustered]

args = pd.DataFrame({'argumentosKMeans': args}, index=[
                    'NumGrupos', 'numVar', 'numLinhasTreino', 'DadosPreAgrupados'])

# Objetivo : gerar: arquivo com argumentos K Means; datasets completo; dataset treino; agrupamento original treino; dataset teste.

args.to_csv(
    'argsKmeans.txt', sep='\t', index=True, header=False)

data.to_csv(
    'dataSets/dataset_completoKM.txt', sep='\t', index=False)

data_Treino.to_csv('dataSets/dataset_treinoKM.txt', sep='\t',
                   index=True, header=True)


data_Teste.to_csv('dataSets/dataset_testeKM.txt',
                  sep='\t', index=True, header=True)

# Deleçao de arquivos de agrupamento anteriores.

os.remove('Resultado_PlotKmeans/grupo1.txt')
os.remove('Resultado_PlotKmeans/grupo2.txt')
os.remove('Resultado_PlotKmeans/grupo3.txt')
os.remove('Resultado_PlotKmeans/grupo4.txt')
os.remove('Resultado_PlotKmeans/grupo5.txt')
